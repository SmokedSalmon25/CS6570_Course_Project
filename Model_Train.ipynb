{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YXgwLq5Ikl6",
        "outputId": "bea85fdf-f153-4139-ad1d-ea7308029d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/numpy-config\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy-2.0.2.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libgfortran-040039e1-0352e75f.so.5.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libquadmath-96973f99-934c22de.so.0.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libscipy_openblas64_-99b71e71.so\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled numpy-2.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "xDGvKnu-In89",
        "outputId": "3cd44b68-c669-417f-c70d-8976627bfb4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.25.1\n",
            "  Downloading numpy-1.25.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.25.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.1 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.25.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "bc1525e57f97422982d98dcbf6bfa0ac"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install numpy==1.25.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CTHjvMSHrdVs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.nn.init as init\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 HL NN"
      ],
      "metadata": {
        "id": "deczEishjmVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITz9f2Ix9r6C",
        "outputId": "ed008c3d-2984-45e3-ffb6-ab71b46368c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 0.6615\n",
            "Epoch 2 - Loss: 0.6205\n",
            "Epoch 3 - Loss: 0.5212\n",
            "Epoch 4 - Loss: 0.4874\n",
            "Epoch 5 - Loss: 0.3988\n",
            "Epoch 6 - Loss: 0.3411\n",
            "Epoch 7 - Loss: 0.2797\n",
            "Epoch 8 - Loss: 0.2649\n",
            "Epoch 9 - Loss: 0.2725\n",
            "Epoch 10 - Loss: 0.2047\n",
            "Hard-label accuracy: 99.67%\n"
          ]
        }
      ],
      "source": [
        "# === Config ===\n",
        "hidden_size = 2\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "# === Data ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to float32 in [0, 1]\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),  # Normalize with MNIST mean and std\n",
        "    transforms.Lambda(lambda x: x / 100),  # Divide by 100 after normalization\n",
        "    transforms.Lambda(lambda x: x.view(-1).double())  # Flatten and convert to float64\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Filter only digits 0 and 1\n",
        "train_idx = [i for i, (_, y) in enumerate(train_data) if y in [0, 1]]\n",
        "test_idx  = [i for i, (_, y) in enumerate(test_data)  if y in [0, 1]]\n",
        "\n",
        "train_loader = DataLoader(Subset(train_data, train_idx), batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(Subset(test_data,  test_idx),  batch_size=batch_size)\n",
        "\n",
        "# === Model ===\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, hidden_size, dtype=torch.float64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, 1, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = MyModel(hidden_size).double()  # instantiate with hidden_size\n",
        "\n",
        "# === Training ===\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.double()\n",
        "        yb = yb.double().unsqueeze(1)  # (batch_size, 1)\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.double()\n",
        "        out = model(xb)\n",
        "        pred = (out > 0).int().squeeze()\n",
        "        correct += (pred == yb.int()).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "print(f\"Hard-label accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# === Save weights & biases ===\n",
        "weights = [\n",
        "    model.fc1.weight.detach().cpu().numpy(),        # shape: (2, 784)\n",
        "    model.fc2.weight.detach().cpu().numpy()         # shape: (1, 2)\n",
        "]\n",
        "biases = [\n",
        "    model.fc1.bias.detach().cpu().numpy().reshape(-1, 1),  # shape: (2, 1)\n",
        "    model.fc2.bias.detach().cpu().numpy().reshape(-1, 1)   # shape: (1, 1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "rJe6HnMXoNeU",
        "outputId": "600803bb-96bd-43c6-bf23-1742c22924cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[ 0.03100247, -0.00498552,  0.02210274, ...,  0.0108135 ,\n",
            "        -0.02854328, -0.00452127],\n",
            "       [-0.54996544, -0.49725528, -0.50737784, ..., -0.5460584 ,\n",
            "        -0.52039397, -0.49743261]]), array([[0.45798796, 1.34748298]])]\n"
          ]
        }
      ],
      "source": [
        "print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "YgWZaItUFHL7",
        "outputId": "ecef6190-b1d2-4bab-acf6-d6b56ab99202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/relu_mnist_hidden2_try_2.npz\n"
          ]
        }
      ],
      "source": [
        "# Save as .npz with arr_0 (weights), arr_1 (biases)\n",
        "save_path = \"./relu_mnist_hidden2_try_2.npz\"\n",
        "np.savez(save_path,\n",
        "         arr_0=np.array(weights, dtype=object),\n",
        "         arr_1=np.array(biases, dtype=object))\n",
        "\n",
        "print(f\"Model saved to: {os.path.abspath(save_path)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOMunpemobuc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH3_TWxDCZeC"
      },
      "source": [
        "#Two Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uaz3JjnCZHD",
        "outputId": "acda2736-72aa-4fb2-93c9-3c954512cd89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 0.7143\n",
            "Epoch 2 - Loss: 0.6940\n",
            "Epoch 3 - Loss: 0.6587\n",
            "Epoch 4 - Loss: 0.5763\n",
            "Epoch 5 - Loss: 0.4445\n",
            "Epoch 6 - Loss: 0.3881\n",
            "Epoch 7 - Loss: 0.3625\n",
            "Epoch 8 - Loss: 0.2573\n",
            "Epoch 9 - Loss: 0.2493\n",
            "Epoch 10 - Loss: 0.2194\n",
            "Hard-label accuracy: 99.34%\n",
            "✅ Model saved to relu_2_classes.npz\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "# === Config ===\n",
        "hidden_size = 2\n",
        "output_size = 2   # softmax outputs before collapsing\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "model_path = \"relu_2_classes.npz\"  # save filename\n",
        "\n",
        "# === Data ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to float32 in [0, 1]\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),  # Normalize with MNIST mean and std\n",
        "    transforms.Lambda(lambda x: x / 100),  # Divide by 100 after normalization\n",
        "    transforms.Lambda(lambda x: x.view(-1).double())  # Flatten and convert to float64\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Filter only digits 0 and 1\n",
        "train_idx = [i for i, (_, y) in enumerate(train_data) if y in [0, 1]]\n",
        "test_idx  = [i for i, (_, y) in enumerate(test_data)  if y in [0, 1]]\n",
        "\n",
        "train_loader = DataLoader(Subset(train_data, train_idx), batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(Subset(test_data,  test_idx),  batch_size=batch_size)\n",
        "\n",
        "# === Model ===\n",
        "class TwoOutputNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, hidden_size, dtype=torch.float64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # raw logits\n",
        "        return x\n",
        "\n",
        "model = TwoOutputNN().double()  # ensure all params are float64\n",
        "\n",
        "# === Training ===\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.double()        # cast inputs to float64\n",
        "        yb = yb.long()          # for CrossEntropyLoss\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.double()\n",
        "        out = model(xb)\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "print(f\"Hard-label accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# === Collapse output: class1 - class0 ===\n",
        "fc2_weight = model.fc2.weight.detach().cpu().numpy()       # shape: (2, 2)\n",
        "fc2_bias   = model.fc2.bias.detach().cpu().numpy().reshape(-1, 1)  # shape: (2, 1)\n",
        "\n",
        "# Compute final collapsed layer: w = w1 - w0, b = b1 - b0\n",
        "collapsed_w = (fc2_weight[1] - fc2_weight[0]).reshape(1, hidden_size)  # (1, 2)\n",
        "collapsed_b = (fc2_bias[1]   - fc2_bias[0]).reshape(1, 1)              # (1, 1)\n",
        "\n",
        "# === Save as [784 → 2 → 1] model ===\n",
        "weights = [\n",
        "    model.fc1.weight.detach().cpu().numpy(),  # shape: (2, 784)\n",
        "    collapsed_w                               # shape: (1, 2)\n",
        "]\n",
        "biases = [\n",
        "    model.fc1.bias.detach().cpu().numpy().reshape(-1, 1),  # shape: (2, 1)\n",
        "    collapsed_b                                            # shape: (1, 1)\n",
        "]\n",
        "\n",
        "np.savez(model_path,\n",
        "         arr_0=np.array(weights, dtype=object),\n",
        "         arr_1=np.array(biases, dtype=object))\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_4b7QvHDumc"
      },
      "outputs": [],
      "source": [
        "fcn = np.load('/content/2_classes_784_2_1.npz', allow_pickle=True)\n",
        "weights, biases = fcn['ws'], fcn['bs']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_off = weights[1]"
      ],
      "metadata": {
        "id": "RSfYt4eWwOEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_off = biases[1]"
      ],
      "metadata": {
        "id": "8AEmhwiQwmgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dimensions\n",
        "d_in = 2     # example input dimension\n",
        "d_hidden = 1   # number of hidden units\n",
        "\n",
        "# Initialize w1 and b1 from N(0, 1)\n",
        "w1 = np.random.normal(loc=0.0, scale=1.0, size=(d_hidden, d_in))\n",
        "b1 = np.random.normal(loc=0.0, scale=1.0, size=(d_hidden, 1))\n",
        "\n",
        "print(w1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEYhz3mPwpI1",
        "outputId": "62859bef-3412-4f62-f110-28f7d04fb1ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.27924659  0.36512713]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2 = w1 + w_off\n",
        "print(w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLzJNq-8xoDd",
        "outputId": "af8fb4a3-edfa-438a-8250-6249f51167db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.77700951 -0.13263571]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b2 = b1 + b_off"
      ],
      "metadata": {
        "id": "ux5XSJJAyQpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_2nd_layer = np.vstack([w1, w2])"
      ],
      "metadata": {
        "id": "zw1qi1vvydVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biases_2nd_layer = np.vstack([b1, b2])"
      ],
      "metadata": {
        "id": "uNztiKDizaPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(biases_2nd_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me5kerr4yrKY",
        "outputId": "56775660-94d7-458c-c193-e168a8441a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.90644664]\n",
            " [-0.7248949 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print"
      ],
      "metadata": {
        "id": "pDYTJBRIzp5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wights_1st_layer = weights[0]\n",
        "biases_1st_layer = biases[0]"
      ],
      "metadata": {
        "id": "UJbwD6D9ysqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_extract = TwoOutputNN()\n",
        "\n",
        "model_extract.fc1.weight.data = torch.from_numpy(wights_1st_layer).double()\n",
        "model_extract.fc1.bias.data   = torch.from_numpy(biases_1st_layer.reshape(-1)).double()\n",
        "\n",
        "model_extract.fc2.weight.data = torch.from_numpy(weight_2nd_layer).double()\n",
        "model_extract.fc2.bias.data   = torch.from_numpy(biases_2nd_layer.reshape(-1)).double()"
      ],
      "metadata": {
        "id": "63v2I2rby4fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"fc1.weight.shape:\", model_extract.fc1.weight.data.shape, \"(expected:\", (hidden_size, 784), \")\")\n",
        "print(\"fc1.bias.shape:  \", model_extract.fc1.bias.data.shape,   \"(expected:\", (hidden_size,), \")\")\n",
        "\n",
        "print(\"fc2.weight.shape:\", model_extract.fc2.weight.data.shape, \"(expected:\", (output_size, hidden_size), \")\")\n",
        "print(\"fc2.bias.shape:  \", model_extract.fc2.bias.data.shape,   \"(expected:\", (output_size,), \")\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPdW1Uq4zzvq",
        "outputId": "e8d7d663-fa13-4e36-ea70-440f2b9f1fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1.weight.shape: torch.Size([2, 784]) (expected: (2, 784) )\n",
            "fc1.bias.shape:   torch.Size([2]) (expected: (2,) )\n",
            "fc2.weight.shape: torch.Size([2, 2]) (expected: (2, 2) )\n",
            "fc2.bias.shape:   torch.Size([2]) (expected: (2,) )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Evaluation ===\n",
        "model_extract.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.double()\n",
        "        out = model_extract(xb)\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "print(f\"Hard-label accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7ccuUZvzgsx",
        "outputId": "b4c32ab0-0ce9-4843-f7bb-6f337db648ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Hard-label accuracy: 99.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8MTl4j2yzmlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Three Classes"
      ],
      "metadata": {
        "id": "UJ4dEpwP0dRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "# === Config ===\n",
        "hidden_size = 2\n",
        "output_size = 3  # 3 classes: 0, 1, 3 mapped to 0, 1, 2\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "# === Label mapping function ===\n",
        "def remap_targets(dataset, label_map):\n",
        "    targets = np.array(dataset.targets)\n",
        "    mapped_indices = np.where(np.isin(targets, list(label_map.keys())))[0]\n",
        "    dataset.targets = torch.tensor([label_map[int(y)] for y in targets[mapped_indices]])\n",
        "    dataset.data = dataset.data[mapped_indices]\n",
        "    return dataset\n",
        "\n",
        "# === Transform ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    transforms.Lambda(lambda x: x / 100),\n",
        "    transforms.Lambda(lambda x: x.view(-1).double())\n",
        "])\n",
        "\n",
        "# === Load Data ===\n",
        "label_map = {0: 0, 1: 1, 3: 2}  # map original labels to 0-based\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_data = remap_targets(train_data, label_map)\n",
        "test_data  = remap_targets(test_data, label_map)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# === Model ===\n",
        "class TwoOutputNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, hidden_size, dtype=torch.float64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = TwoOutputNN().double()\n",
        "\n",
        "# === Training ===\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.double()\n",
        "        yb = yb.long()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.double()\n",
        "        out = model(xb)\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "print(f\"Hard-label accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNDOVx2x0ee5",
        "outputId": "28f37110-dbd9-45f9-fad4-20068bb157ec"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 1.0964\n",
            "Epoch 2 - Loss: 0.9889\n",
            "Epoch 3 - Loss: 0.8386\n",
            "Epoch 4 - Loss: 0.7846\n",
            "Epoch 5 - Loss: 0.7155\n",
            "Epoch 6 - Loss: 0.6398\n",
            "Epoch 7 - Loss: 0.6501\n",
            "Epoch 8 - Loss: 0.5374\n",
            "Epoch 9 - Loss: 0.5722\n",
            "Epoch 10 - Loss: 0.4953\n",
            "Hard-label accuracy: 73.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fc2_weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JabmNBA11EJ",
        "outputId": "b8c45bbc-5092-4f50-89ac-3e7cd23b0da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Collapse output: class1 - class0 ===\n",
        "fc2_weight = model.fc2.weight.detach().cpu().numpy()       # shape: (2, 2)\n",
        "fc2_bias   = model.fc2.bias.detach().cpu().numpy().reshape(-1, 1)  # shape: (2, 1)\n",
        "\n",
        "# Compute final collapsed layer: w = w1 - w0, b = b1 - b0\n",
        "collapsed_w = (fc2_weight[0] - fc2_weight[1]).reshape(1, hidden_size)  # (1, 2)\n",
        "collapsed_b = (fc2_bias[0]   - fc2_bias[1]).reshape(1, 1)              # (1, 1)\n",
        "\n",
        "# === Save as [784 → 2 → 1] model ===\n",
        "weights = [\n",
        "    model.fc1.weight.detach().cpu().numpy(),  # shape: (2, 784)\n",
        "    collapsed_w                               # shape: (1, 2)\n",
        "]\n",
        "biases = [\n",
        "    model.fc1.bias.detach().cpu().numpy().reshape(-1, 1),  # shape: (2, 1)\n",
        "    collapsed_b                                            # shape: (1, 1)\n",
        "]\n",
        "\n",
        "np.savez('class1-2_3classes',\n",
        "         arr_0=np.array(weights, dtype=object),\n",
        "         arr_1=np.array(biases, dtype=object))\n",
        "\n",
        "print(f\"Model saved to class1-2_3classes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlzyzeYt0mIb",
        "outputId": "cf6b3d4f-b785-4ca1-d728-885d315adf2f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to class1-2_3classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Collapse output: class1 - class0 ===\n",
        "fc2_weight = model.fc2.weight.detach().cpu().numpy()       # shape: (2, 2)\n",
        "fc2_bias   = model.fc2.bias.detach().cpu().numpy().reshape(-1, 1)  # shape: (2, 1)\n",
        "\n",
        "# Compute final collapsed layer: w = w1 - w0, b = b1 - b0\n",
        "collapsed_w = (fc2_weight[1] - fc2_weight[2]).reshape(1, hidden_size)  # (1, 2)\n",
        "collapsed_b = (fc2_bias[1]   - fc2_bias[2]).reshape(1, 1)              # (1, 1)\n",
        "\n",
        "# === Save as [784 → 2 → 1] model ===\n",
        "weights = [\n",
        "    model.fc1.weight.detach().cpu().numpy(),  # shape: (2, 784)\n",
        "    collapsed_w                               # shape: (1, 2)\n",
        "]\n",
        "biases = [\n",
        "    model.fc1.bias.detach().cpu().numpy().reshape(-1, 1),  # shape: (2, 1)\n",
        "    collapsed_b                                            # shape: (1, 1)\n",
        "]\n",
        "\n",
        "np.savez('class2-3_3classes',\n",
        "         arr_0=np.array(weights, dtype=object),\n",
        "         arr_1=np.array(biases, dtype=object))\n",
        "\n",
        "print(f\"Model saved to class2-3_3classes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRWhNRAx1dis",
        "outputId": "8f51f731-6ee8-4d0f-8e06-61a19b062b30"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to class2-3_3classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPl71qiU3nFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking leaky_relu\n"
      ],
      "metadata": {
        "id": "a0rx0DWk9jzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load .npz file\n",
        "data = np.load('./leaky_alpha_0.1.npz', allow_pickle=True)\n",
        "\n",
        "# Inspect contents\n",
        "print(\"Keys:\", data.files)\n",
        "ws = data['ws']  # list of weight matrices\n",
        "bs = data['bs']  # list of bias vectors\n",
        "\n",
        "# Let's print their shapes\n",
        "for i, (w, b) in enumerate(zip(ws, bs)):\n",
        "    print(f\"Layer {i}: W.shape = {w.shape}, b.shape = {b.shape}\")\n",
        "\n",
        "# Transpose weight matrices\n",
        "W1 = ws[0].T  # (784, 2)\n",
        "W2 = ws[1].T  # (2, 1)\n",
        "\n",
        "# Flatten biases\n",
        "b1 = bs[0].reshape(-1)  # (2,)\n",
        "b2 = bs[1].reshape(-1)  # (1,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GExjU8hO9qao",
        "outputId": "9ed8294c-e223-4cbb-ca5b-8e8ea433f81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: ['ws', 'bs']\n",
            "Layer 0: W.shape = (2, 784), b.shape = (2, 1)\n",
            "Layer 1: W.shape = (1, 2), b.shape = (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def predict(X):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "    return a2\n"
      ],
      "metadata": {
        "id": "5lplXT2g-jsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(_, _), (x_test, y_test) = mnist.load_data()\n",
        "x_test = x_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "\n",
        "# Keep only 0 and 1\n",
        "mask = (y_test == 0) | (y_test == 1)\n",
        "x_test = x_test[mask]\n",
        "y_test = y_test[mask].reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "OfaKC6Bh-4EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(x_test)\n",
        "y_pred_labels = (y_pred >= 0.5).astype(int)\n",
        "\n",
        "accuracy = np.mean(y_pred_labels == y_test)\n",
        "print(f\"Accuracy on MNIST (0 vs 1): {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CAHkJ4m-5xG",
        "outputId": "a029ad1b-6eb0-4e5b-88f7-278627b73fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on MNIST (0 vs 1): 53.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d43ef812e831>:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from google.colab import files\n",
        "\n",
        "# # === Upload the .npz file ===\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# === Load weights and biases from .npz ===\n",
        "data = np.load(\"leaky_alpha_0.1.npz\", allow_pickle=True)\n",
        "W1 = data['ws'][0]               # shape: (2, 784)\n",
        "W2 = data['ws'][1]               # shape: (1, 2)\n",
        "b1 = data['bs'][0].reshape(-1)   # shape: (2,)\n",
        "b2 = data['bs'][1].reshape(-1)   # shape: (1,)\n",
        "\n",
        "# === Define activation functions ===\n",
        "def leaky_relu(x, alpha=0.1):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# === Forward pass / prediction ===\n",
        "def predict(X):\n",
        "    z1 = np.dot(X, W1.T) + b1          # Hidden layer: (N, 2)\n",
        "    a1 = leaky_relu(z1, alpha=0.01)\n",
        "    z2 = np.dot(a1, W2.T) + b2         # Output layer: (N, 1)\n",
        "    a2 = sigmoid(z2)                  # Convert logits to probability\n",
        "    return a2\n",
        "\n",
        "# === Data preprocessing ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),  # Normalize using MNIST mean and std\n",
        "    transforms.Lambda(lambda x: x / 100),        # Divide by 100 (as per training)\n",
        "    transforms.Lambda(lambda x: x.view(-1).double())  # Flatten and convert to float64\n",
        "])\n",
        "\n",
        "# Load MNIST test data (only digits 0 and 1)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_idx = [i for i, (_, y) in enumerate(test_data) if y in [0, 1]]\n",
        "filtered_test = Subset(test_data, test_idx)\n",
        "\n",
        "# Prepare test dataset arrays\n",
        "x_test, y_test = [], []\n",
        "for i in range(len(filtered_test)):\n",
        "    x, y = filtered_test[i]\n",
        "    x_test.append(x.numpy())\n",
        "    y_test.append(y)\n",
        "\n",
        "x_test = np.array(x_test)     # shape: (N, 784)\n",
        "y_test = np.array(y_test)     # shape: (N,)\n",
        "\n",
        "# === Predict and evaluate accuracy ===\n",
        "y_pred = predict(x_test)\n",
        "y_pred_labels = (y_pred >= 0.5).astype(int).reshape(-1)\n",
        "accuracy = np.mean(y_pred_labels == y_test)\n",
        "\n",
        "print(f\"✅ Accuracy on MNIST (digits 0 vs 1): {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq6tH1H1-7w7",
        "outputId": "c7c05e2a-79c7-4960-ee4b-cd1280f9bb3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Accuracy on MNIST (digits 0 vs 1): 99.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Layer Deep Networks"
      ],
      "metadata": {
        "id": "o6X3nNUiD9gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# === Config ===\n",
        "hidden_sizes = [2, 2]\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "# === Data ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.CenterCrop(6),  # From 28x28 to 6x6\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    transforms.Lambda(lambda x: x / 100),  # Divide by 100 after normalization\n",
        "    transforms.Lambda(lambda x: x.view(-1)[:32].double())  # Flatten and take first 32 pixels\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Filter only digits 0 and 1\n",
        "train_idx = [i for i, (_, y) in enumerate(train_data) if y in [0, 1]]\n",
        "test_idx  = [i for i, (_, y) in enumerate(test_data)  if y in [0, 1]]\n",
        "\n",
        "train_loader = DataLoader(Subset(train_data, train_idx), batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(Subset(test_data,  test_idx),  batch_size=batch_size)\n",
        "\n",
        "# === Model ===\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(32, hidden_sizes[0], dtype=torch.float64)\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1], dtype=torch.float64)\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], 1, dtype=torch.float64)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = MyModel().double()\n",
        "\n",
        "# === Training ===\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.double()\n",
        "        yb = yb.double().unsqueeze(1)\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.double()\n",
        "        out = model(xb)\n",
        "        pred = (out > 0).int().squeeze()\n",
        "        correct += (pred == yb.int()).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "print(f\"Hard-label accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# === Save weights & biases ===\n",
        "weights = [\n",
        "    model.fc1.weight.detach().cpu().numpy(),\n",
        "    model.fc2.weight.detach().cpu().numpy(),\n",
        "    model.fc3.weight.detach().cpu().numpy()\n",
        "]\n",
        "biases = [\n",
        "    model.fc1.bias.detach().cpu().numpy().reshape(-1, 1),\n",
        "    model.fc2.bias.detach().cpu().numpy().reshape(-1, 1),\n",
        "    model.fc3.bias.detach().cpu().numpy().reshape(-1, 1)\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivs-sc9oEAtH",
        "outputId": "8b085402-9ca5-4e4c-bdf3-bd586e6596fa"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 0.6779\n",
            "Epoch 2 - Loss: 0.6719\n",
            "Epoch 3 - Loss: 0.6459\n",
            "Epoch 4 - Loss: 0.6316\n",
            "Epoch 5 - Loss: 0.5659\n",
            "Epoch 6 - Loss: 0.4943\n",
            "Epoch 7 - Loss: 0.4257\n",
            "Epoch 8 - Loss: 0.3894\n",
            "Epoch 9 - Loss: 0.3259\n",
            "Epoch 10 - Loss: 0.2659\n",
            "Hard-label accuracy: 99.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as .npz with arr_0 (weights), arr_1 (biases)\n",
        "save_path = \"./relu_mnist_hidden_32_2_2_1.npz\"\n",
        "np.savez(save_path,\n",
        "         arr_0=np.array(weights, dtype=object),\n",
        "         arr_1=np.array(biases, dtype=object))\n",
        "\n",
        "print(f\"Model saved to: {os.path.abspath(save_path)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyPnqO97EaJX",
        "outputId": "71859d8f-3be3-47ff-818c-99bd8926d23e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/relu_mnist_hidden_32_2_2_1.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azIzGUyVGMDF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HH3_TWxDCZeC"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}